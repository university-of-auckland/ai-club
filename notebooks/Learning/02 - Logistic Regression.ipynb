{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise has been taken from the [Stanford Machine Learning Course](https://github.com/krasserm/machine-learning-notebooks). With the original exercise written for Octave available [here](https://github.com/krasserm/machine-learning-notebooks/blob/master/data/ml-ex2/ex2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import colors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unregularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants\n",
    "that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.Your task is to build a classification model that estimates an applicant’s\n",
    "probability of admission based the scores from those two exams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admission data: \n",
    "# - exam 1 score (x1) \n",
    "# - exam 2 score (x2)\n",
    "# - admitted (y)\n",
    "data = np.loadtxt('/aiuoa/datasets/students_1.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (x1, x2) from target (y)\n",
    "X, y = np.hsplit(data, np.array([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression estimator expect an y row vector\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualise the data that has been provided to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Mask for selecting positive and negative examples\n",
    "y_pos = y == 1\n",
    "y_neg = y == 0\n",
    "\n",
    "# Plot examples and decision boundary\n",
    "ax.plot(X[y_pos,0], X[y_pos,1], 'b+', label='Admitted')\n",
    "ax.plot(X[y_neg,0], X[y_neg,1], 'yo', label='Not admitted')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'lbfgs' solver for logistic regression as this is what Octave fminunc does.\n",
    "# Parameter C is the inverse regularization strength (high values = low regularization).\n",
    "clf = LogisticRegression(C=1e9, solver='lbfgs')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = clf.intercept_[0]\n",
    "theta1 = clf.coef_[0,0]\n",
    "theta2 = clf.coef_[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes x2 at y=0.5 from x1 and model parameters\n",
    "# (used for computing the linear decision boundary)\n",
    "def x2(x1):\n",
    "    return (0.5 - theta0 - theta1*x1) / theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_min = X[:,0].min()\n",
    "x1_max = X[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 and x2 data of linear decision boundary\n",
    "x1_plot = np.array([x1_min, x1_max])\n",
    "x2_plot = x2(x1_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Mask for selecting positive and negative examples\n",
    "y_pos = y == 1\n",
    "y_neg = y == 0\n",
    "\n",
    "# Plot examples and decision boundary\n",
    "ax.plot(X[y_pos,0], X[y_pos,1], 'b+', label='Admitted')\n",
    "ax.plot(X[y_neg,0], X[y_neg,1], 'yo', label='Not admitted')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Plot decision boundary\n",
    "ax.plot(x1_plot, x2_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([\n",
    "    [45., 85.],\n",
    "    [50., 50.],\n",
    "    [80., 80.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities\n",
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = clf.intercept_[0]\n",
    "theta1 = clf.coef_[0,0]\n",
    "theta2 = clf.coef_[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0, theta1, theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy on training set\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure\n",
    "it is functioning correctly. Suppose you are the product manager of the factory and you have the test results for some microchips on two di↵erent tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microchip test: \n",
    "# - test 1 (x1) \n",
    "# - test 2 (x2)\n",
    "# - accepted=1, rejected=0 (y)\n",
    "data = np.loadtxt('/aiuoa/datasets/students_2.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (x1, x2) from target (y)\n",
    "X, y = np.hsplit(data, np.array([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression estimator requires an y row vector\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Mask for selecting positive and negative examples\n",
    "y_pos = y == 1\n",
    "y_neg = y == 0\n",
    "\n",
    "# Plot examples\n",
    "ax.plot(X[y_pos,0], X[y_pos,1], 'b+', label='Accepted')\n",
    "ax.plot(X[y_neg,0], X[y_neg,1], 'yo', label='Rejected')\n",
    "ax.set_xlabel('Microchip test 1')\n",
    "ax.set_ylabel('Microchip test 2')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor to include polynomial features up to degree 6\n",
    "poly = PolynomialFeatures(6, include_bias=False)\n",
    "\n",
    "# Mean and standard deviation scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Logistic regression classifier. \n",
    "# - C=1.0 will result in good fit\n",
    "# - C=1e4 will result in overfit (to little regularization)\n",
    "# - C=1e-2 will result in underfit (to much regularization)\n",
    "clf = LogisticRegression(C=1.0, solver='lbfgs')\n",
    "\n",
    "# Pipeline of polynomial feature generator, feature scaler and linear regressor\n",
    "model = Pipeline([('poly', poly), ('scaler', scaler), ('clf', clf)])\n",
    "\n",
    "# Fit data to model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy on training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = np.mgrid[-1:1:500j, -1:1:500j]\n",
    "\n",
    "# Compute acceptance probabilities on 500*500 grid \n",
    "X_grid = np.c_[grid[0].ravel(), grid[1].ravel()]\n",
    "y_grid = model.predict(X_grid).reshape(grid[0].shape)\n",
    "\n",
    "# Plot decision boundary on previous figure\n",
    "cs = ax.contour(grid[0], grid[1], y_grid, 'g-', levels=[0.5])\n",
    "ax.clabel(cs)\n",
    "\n",
    "# Show previous figure with decision boundary\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
