{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are the workhorse of modern machine learning algorithms. They are based on how a human brain works and operates. They utilise statistics and derivative mathematics to discover the relationship between inputs and outputs. In this Jupyter Notebook we will explore some of the underlying principles behind how neural networks work and slowly unfold some of the complex mathematics that underpins them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. \n",
    "\n",
    "There are three key components that make up the neural network, these are the activation function, the weights and biases.\n",
    "- An activation function is a mathematical function that applies a mathematical formula to all of the inputs into the neuron to produce some (usually non-linear output).\n",
    "- The weights are the multiplier applied to the connection between neurons that modify the values of the output of one neuron into the input of the next neuron.\n",
    "- Biases are added or subtracted from neurons to shift the activation function.\n",
    "\n",
    "These three properties of a neural network can be easily shown in the image below:\n",
    "![](images/neurons.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going in depth into how neural networks work, it is important to understand the concept of 'feed forward' in the context of neural networks. To help explain the concept of feed forward, we will use the previous 'students' dataset to find the relationship between exam score 1, 2, and whether they will be admitted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.utils\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we will import the dataset. This time though, we will convert the 'y' field into two categories. This essentially makes two new columns 'Not Admitted' and 'Admitted', so when 'y' was previously a 0 this will now correspond to a 1 in the 'Not Admitted' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Admission data: \n",
    "# - exam 1 score (x1) \n",
    "# - exam 2 score (x2)\n",
    "# - admitted (y)\n",
    "data = np.loadtxt('/aiuoa/datasets/students_1.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Separate features (x1, x2) from target (y)\n",
    "X, y = np.hsplit(data, np.array([2]))\n",
    "y = keras.utils.to_categorical(y)\n",
    "y_shape = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train a simple categorical neural network on the data given above utilising the sigmoid function that was showcased when looking at the logistic regression example. ![](images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we have declared a neural network that utilises the sigmoid function and only connects the input layer to the output layer. This results in a pretty bad neural network that appears to be no better than flipping a coin to determine whether a student was admitted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Output layer\n",
    "model.add(Dense(2, activation='sigmoid', input_dim=2))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this neural network using the VisualizeNN script which shows the relationship between the neurons and the weights. (Note this visualisation tool does not show biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import VisualizeNN as VisNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Neural Network with weights\n",
    "network_structure = np.hstack(([X.shape[1]], [y_shape]))\n",
    "weights = []\n",
    "for i in range(0, len(model.get_weights())):\n",
    "    if \"bias\" not in model.weights[i].name:\n",
    "        weights.append(model.get_weights()[i])\n",
    "network = VisNN.DrawNN(network_structure, weights)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have shown that connecting the input straight to the output does not yield promising results, let us intoduce a hidden layer with three neurons to see if we can train this network to find the relationship between the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Hidden Layer 1\n",
    "model.add(Dense(3, activation='sigmoid', input_dim=2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=5000, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to verify the model structure before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get layer size.\n",
    "layer_size = []\n",
    "for layer in model.layers:\n",
    "    layer_size.append(int(layer.get_output_at(0).shape[1]))\n",
    "layer_size.pop()\n",
    "print(layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can visualise the network (without biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the Neural Network with weights\n",
    "network_structure = np.hstack(([X.shape[1]], np.asarray(layer_size), [y_shape]))\n",
    "weights = []\n",
    "for i in range(0, len(model.get_weights())):\n",
    "    if \"bias\" not in model.weights[i].name:\n",
    "        weights.append(model.get_weights()[i])\n",
    "network = VisNN.DrawNN(network_structure, weights)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the weight values of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can verify that the network is working as expected by placing in some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[25, 25]])\n",
    "print('true: 0, predicted:' + str(model.predict_classes(test)))\n",
    "test = np.array([[100, 100]])\n",
    "print('true: 1, predicted:' + str(model.predict_classes(test)))\n",
    "test = np.array([[50, 50]])\n",
    "print('true: 1/0, predicted:' + str(model.predict_classes(test)))\n",
    "test = np.array([[60, 60]])\n",
    "print('true: 1, predicted:' + str(model.predict_classes(test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
